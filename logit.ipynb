{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a59a857-f557-4562-a11e-f38cd5338b4a",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c309e0-2488-4203-b284-54d40eb9be9d",
   "metadata": {},
   "source": [
    "Batch Size: In machine learning, the training process involves updating the model's parameters based on the gradients computed from a batch of training samples. The batch size determines the number of samples that are processed together before the model's parameters are updated.\n",
    "\n",
    "For example, with a dataset of 1000 samples and a batch size of 100, there would be a total of 10 iterations or updates of the model's parameters.\n",
    "\n",
    "Reason behind the need of batch size: \n",
    "* Mmeory Efficient: batch size smaller than the entire datast\n",
    "* Computational Efficiency: Batch processing can take advantage of parallelism in modern hardware, such as GPUs. By processing multiple samples simultaneously, the computations can be distributed across multiple cores or devices, leading to faster training times.\n",
    "* Parameters to be updated more frequently, letting the model to converge to the local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3888de-c51c-4d7a-a66f-c43dce36f273",
   "metadata": {},
   "source": [
    "* A larger batch size = more stable gradient estimates but computationally more expensive\n",
    "* Smaller batch size = more frequent update of model parameters and potentiall converge faster but introduce more noise in the estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece02cc2-0e8e-4db5-9bd1-e8305f5cf1b6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10cf7b82-7b4d-40c1-ad56-a98a590f3953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f14ca114-bf42-4efc-b512-88381ffc59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Framework\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self,input_size):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size,1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.linear(x)\n",
    "        out = self.sigmoid(out) # probabilities\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8467e489-960a-4f5e-abc2-dcd2ec559f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.9164\n",
      "Epoch [20/100], Loss: 0.7192\n",
      "Epoch [30/100], Loss: 0.6488\n",
      "Epoch [40/100], Loss: 0.6239\n",
      "Epoch [50/100], Loss: 0.6136\n",
      "Epoch [60/100], Loss: 0.6082\n",
      "Epoch [70/100], Loss: 0.6045\n",
      "Epoch [80/100], Loss: 0.6015\n",
      "Epoch [90/100], Loss: 0.5986\n",
      "Epoch [100/100], Loss: 0.5959\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset\n",
    "X = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [4.0, 5.0]])\n",
    "Y = torch.tensor([[0.0], [0.0], [1.0], [1.0]])\n",
    "\n",
    "# Model hyperparameters\n",
    "class Params(object):\n",
    "    def __init__(self,input_size,learning_rate,epochs, threshold):\n",
    "        self.input_size = input_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.threshold = threshold\n",
    "        \n",
    "args = Params(2,0.01,100,0.7)\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(args.input_size)\n",
    "\n",
    "\n",
    "# Define Loss Function and Optimizer \n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=args.learning_rate)\n",
    "\n",
    "# Training_loop\n",
    "for e in range(args.epochs):\n",
    "    # forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "    \n",
    "    # Backword and optimization \n",
    "    \n",
    "    \"\"\" \n",
    "     In PyTorch, when performing backpropagation to compute the gradients of the model's parameters, \n",
    "     it is necessary to zero out the gradients from the previous iteration. \n",
    "     This is because PyTorch accumulates gradients by default, so if we don't reset the gradients, \n",
    "     they would accumulate and interfere with subsequent parameter updates.\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "    loss.backward()\n",
    "    \"\"\"\n",
    "     computes the gradients of the loss function with respect to all the tensors that require gradients in the computational graph. \n",
    "     It essentially performs automatic differentiation and accumulates the gradients in the respective parameters of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    \"\"\"\n",
    "    applies the computed gradients to the model's parameters using the specified optimization algorithm (e.g., SGD, Adam). \n",
    "    It adjusts the parameters in the direction that reduces the loss, allowing the model to learn from the training data.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "     # Print the progress\n",
    "    if (e + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{e+1}/{args.epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a428918-64fa-47ba-a77e-78773e016986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability: 0.7145\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "test_input = torch.tensor([[5.0, 6.0]])\n",
    "predicted = model(test_input)\n",
    "print(f\"Predicted probability: {predicted.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab916695-6460-4fce-b989-f40b8fd77a0e",
   "metadata": {},
   "source": [
    "By default, the threshold is commonly set to 0.5, but you can adjust it according to your specific needs and the trade-off between precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c17770-36dd-4174-99a3-84c788b9f048",
   "metadata": {},
   "source": [
    "#### Changing the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66de605f-0f60-424a-a093-9c72b6f74c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `outputs` contains the predicted probabilities\n",
    "# args.threshold = 0.7  # Set a new threshold value\n",
    "(predicted >= args.threshold).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9d831-ec76-49ba-8404-5b9b182360a2",
   "metadata": {},
   "source": [
    "A higher threshold tends to increase precision (reducing false positives), but it may lead to lower recall (missing some true positives). Conversely, a lower threshold increases recall (capturing more true positives) but may reduce precision (increasing false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59cfe57-d11e-4788-8a68-aa68afdfd249",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
