{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1739a4b-f528-4316-9928-c7bac53d188b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hugging face transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "# ml framework\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Local and Global Importances\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "\n",
    "# plots \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89c0f449-d92a-458e-bf61-a0c16e9dd2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17522, 13)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/reviews.csv')\n",
    "df.head(1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c723164-ad59-4760-ba64-fe495a9c511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalanced\n",
    "\n",
    "def sentiment(score):\n",
    "    if score<= 2:\n",
    "        return 0\n",
    "    elif score == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "df['sentiment'] = df.score.apply(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43201e3-e631-441d-82a8-7675d97d900e",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb01abcd-d764-4548-9e5b-03ad09518ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "\n",
    "# Loading a pre-trained berttokenizer to vectorize our corpus\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d6d6ca8-a5fa-4aed-b080-bd9683d7d142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentence: This is Steve and I am very happy today!\n",
      " Sentence: ['This', 'is', 'Steve', 'and', 'I', 'am', 'very', 'happy', 'today', '!']\n",
      " Sentence: [1188, 1110, 3036, 1105, 146, 1821, 1304, 2816, 2052, 106]\n"
     ]
    }
   ],
   "source": [
    "text = \"This is Steve and I am very happy today!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "for i in [text,tokens,token_id]:\n",
    "    print(f' Sentence: {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e7e07-2adc-4cc1-ae5b-f81962cc884e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Special Tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce073efb-88a0-4288-a331-cae8842bd4a5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ee53d38-a8f2-4913-845c-4126ddc01165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "    text,\n",
    "    # padding=True,\n",
    "    max_length = 32,\n",
    "    add_special_tokens = True,\n",
    "    pad_to_max_length = True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_token_type_ids = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f578b585-0918-493e-9749-bfdf34cb43bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1188, 1110, 3036, 1105,  146, 1821, 1304, 2816, 2052,  106,  102,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba3da8a2-8630-46f4-ae60-c26bf9b00a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the optimal max length for cut off\n",
    "token_lens = []\n",
    "for txt in df.content:\n",
    "    tokens = tokenizer.encode(txt, max_length=512)\n",
    "    token_lens.append(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8059500a-c3f1-480f-b782-d7d8d24368d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Density'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(token_lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58de9a3-be53-4eb3-a292-61c1361e980c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d339dfa-b28a-4f3d-bb59-d8dea7860808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GPReviewDataset():\n",
    "    \n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "  \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "        review,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt')\n",
    "        return {\n",
    "          'review_text': review,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'targets': torch.tensor(target, dtype=torch.long) #sentiments / torch.long for classificaiton problem \n",
    "        }   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "352d4e25-c94f-4eb6-8188-0a1685c56e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=32) #validation\n",
    "df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=32) # actual testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6d00cb2-63b9-4446-8c7f-b7d81bc69cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15769, 14), (876, 14), (877, 14))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22075770-9bbe-48f1-ba7a-633972dbad2e",
   "metadata": {},
   "source": [
    "We also need to create a couple of data loaders. Here's a helper function to do it: Creating a PyTorch Dataset and managing it with Dataloader keeps your data manageable and helps to simplify your machine learning pipeline. a Dataset stores all your data, and Dataloader is can be used to iterate through the data, manage batches, transform the data, and much more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b0b1f39-4cb4-4da1-ac91-efa249a16c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GPReviewDataset(\n",
    "        reviews=df.content.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "      )\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "      )\n",
    "    \n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 160\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85525e0-56e7-4bcc-bd3a-c44eebe01351",
   "metadata": {},
   "source": [
    "Let's have a look at an example batch from our training data loader:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5380adae-3dac-4b9b-a9c9-180da842c8e1",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0e19b9f-3acb-40d3-acad-bd21990ea052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4be16c0-ff2c-4930-bccc-7d963851669d",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state, pooled_output = bert_model(\n",
    "  input_ids=encoding['input_ids'], \n",
    "  attention_mask=encoding['attention_mask']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f87d478-d41d-4087-a795-ace0727a792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_output = bert_model(\n",
    "  input_ids=encoding['input_ids'], \n",
    "  attention_mask=encoding['attention_mask']\n",
    ").pooler_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c03473c0-c904-42d9-9212-d56cfe44dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch model structure\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.bert(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f1f7af3-24d7-4ddd-90f7-2a9a85043abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class_names = ['negative', 'neutral', 'positive']\n",
    "\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1769b5b0-aa0c-44c8-87b6-2ef15a868b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5bfc7-d80e-4950-a1f7-a56d560afd11",
   "metadata": {},
   "source": [
    "How do we come up with all hyperparameters? The BERT authors have some recommendations for fine-tuning:\n",
    "\n",
    "Batch size: 16, 32\n",
    "Learning rate (Adam): 5e-5, 3e-5, 2e-5\n",
    "Number of epochs: 2, 3, 4\n",
    "We're going to ignore the number of epochs recommendation but stick with the rest. Note that increasing the batch size reduces the training time significantly, but gives you lower accuracy.\n",
    "\n",
    "Let's continue with writing a helper function for training our model for one epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23064c27-7375-4261-8bd1-af41174bd2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "  \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"targets\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        # We're avoiding exploding gradients by clipping the gradients of the model using \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c025a57-c40f-4aca-a11c-341f5b11516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask)\n",
    "            \n",
    "            # predictiosn\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39283d69-c84c-4ac1-a8c9-ed6a874ce4e5",
   "metadata": {},
   "source": [
    "Using those two, we can write our training loop. We'll also store the training history:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc35fa-59d1-492a-840b-abf19a557aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "from collections import defaultdict\n",
    "EPOCHS = 3\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,    \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(df_train)\n",
    "      )\n",
    "\n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn, \n",
    "        device, \n",
    "        len(df_val)\n",
    "      )\n",
    "\n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f5ec9-cee9-4dca-aae4-d3c397dc3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2a9a01-cb11-4f0c-89e0-5fbcd9978806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
